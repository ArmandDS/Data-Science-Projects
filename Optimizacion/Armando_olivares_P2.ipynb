{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "<p style=\"text-align:right;\" >Armando Olivares</p><br>\n",
    "Considera los datos usados en clase para el sistema eléctrico español o cualquier otra base de datos con una variable a predecir, al menos otras 5 variables explicativas y un mínimo de 500 observaciones. Queremos ajustar un modelo de regresión lineal mu´ltiple que explique la variable Y en funci´on de las dem´as X (Y = β0X + \u000f) usando “Ridge Regression”:\n",
    "\n",
    "\n",
    "  \\begin{align*}\n",
    "  \\text{minimize}\\quad & ||y-X\\beta||_2^2 +\\rho||\\beta||_2^2\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "### Electricity market data from esios https://www.esios.ree.es/es\n",
    "\n",
    "Data description\n",
    "- price: hourly spot price.\n",
    "- coal: hourly total production coal plants.\n",
    "- gas: hourly total production combined cycle plants.\n",
    "- wind: hourly total production wind plants.\n",
    "- hidro: hourly total production hidro plants.\n",
    "- nuclear: hourly total production nuclear plants.\n",
    "- solar: hourly total production solar plants.\n",
    "- cogen: hourly total production cogeneration plants.\n",
    "- demand: hourly total demand in the market.\n",
    "\n",
    "We want to explain the spot price as a function of the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "price=pd.read_csv('price.csv',delimiter=';',usecols=['value'])\n",
    "coal=pd.read_csv('coal.csv',delimiter=';',usecols=['value'])\n",
    "gas=pd.read_csv('gas.csv',delimiter=';',usecols=['value'])\n",
    "wind=pd.read_csv('wind.csv',delimiter=';',usecols=['value'])\n",
    "hidro=pd.read_csv('hidro.csv',delimiter=';',usecols=['value'])\n",
    "nuclear=pd.read_csv('nuclear.csv',delimiter=';',usecols=['value'])\n",
    "solar=pd.read_csv('solar.csv',delimiter=';',usecols=['value'])\n",
    "cogen=pd.read_csv('cogen.csv',delimiter=';',usecols=['value'])\n",
    "demand=pd.read_csv('demand.csv',delimiter=';',usecols=['value'])\n",
    "df=pd.concat([price, coal, gas, wind, hidro, nuclear, solar, cogen, demand],axis=1)\n",
    "df.columns=['price', 'coal', 'gas', 'wind', 'hidro', 'nuclear', 'solar', 'cogen', 'demand']\n",
    "\n",
    "#df=pd.concat([price, demand],axis=1)\n",
    "#df.columns=['price', 'demand']\n",
    "normalized_df=(df-df.mean())/df.std()\n",
    "#normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separamos y Formateamos Nuestro Datos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[a,b]=df.shape\n",
    "ntest=100\n",
    "df_train = normalized_df.iloc[:(a-ntest), :]\n",
    "df_test = normalized_df.iloc[(a-ntest):, :]\n",
    "# train\n",
    "Y = np.array(df_train['price'])\n",
    "Y = np.expand_dims(Y, axis=1)\n",
    "X0 = np.ones([a-ntest,1])\n",
    "X1 = np.array(df_train[df_train.columns[df_train.columns!='price']])\n",
    "X = np.concatenate([X0, X1],axis=1)\n",
    "#test\n",
    "Y_test = np.array(df_test['price'])\n",
    "Y_test = np.expand_dims(Y_test, axis=1)\n",
    "X0_test = np.ones([ntest,1])\n",
    "X1_test= np.array(df_test[df_test.columns[df_test.columns!='price']])\n",
    "X_test = np.concatenate([X0_test, X1_test],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) (1 punto) Estimar el valor de los parámetros de la regresián aplicando la solución analítica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ecuación para estimar los parametros es de la forma:    $  \\\\beta_{ls}=(X^T X + \\lambda I)^{-1}X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 2085)\n",
      "time elapsed= 0.0007600697936140932\n",
      "[[-0.0115803 ]\n",
      " [ 0.36259694]\n",
      " [ 0.21770513]\n",
      " [-0.06965771]\n",
      " [ 0.36784859]\n",
      " [-0.14696702]\n",
      " [-0.00694018]\n",
      " [ 0.07745367]\n",
      " [-0.04210075]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "rho = 1\n",
    "print (X.T.shape)\n",
    "time_start = time.clock()\n",
    "beta_ls_exact=np.dot(inv(np.dot(X.T,X)+ rho*np.identity(X.shape[1])),np.dot(X.T,Y))\n",
    "time_elapsed = (time.clock() - time_start)\n",
    "print('time elapsed=',time_elapsed)\n",
    "print(beta_ls_exact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el Error de estos Parámetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  18.8163444188\n"
     ]
    }
   ],
   "source": [
    "error = (np.dot(X,beta_ls_exact)- Y)\n",
    "error_MSE =  np.sum(error*error)/(len(X))\n",
    "print(\"MSE: \",error_MSE*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenenos un Error del **18.8%**, en los datos de **Entrenamiento** veamos si podemos mejorar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1.5 puntos) Estimar el valor de los coeﬁcientes de la regresión usando la herramienta minimize del paquete de Python Scipy.optimize. Prueba al menos cuatro solvers diferentes y compara su eﬁciencia en términos de: número de iteraciones totales, número de evaluaciones de la funcón objetivo, gradiente y hesiano, así como el tiempo de computo total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes del utilizar el paquete implementamos las formulas del **ridge regression**, es decir la función objetivo, el jacobiano y el hessiano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def ridge_reg_brute(beta_lb,X,Y,rho):\n",
    "    beta_lb=np.matrix(beta_lb)\n",
    "    z=Y-np.dot(X,beta_lb.T)\n",
    "    return np.dot(z.T,z)+rho*np.dot(beta_lb, beta_lb.T)\n",
    "\n",
    "def ridge_reg_der(beta_ls,X,Y, rho):\n",
    "    beta_ls=np.matrix(beta_ls)\n",
    "    pp = -2*np.dot((Y-np.dot(X,(beta_ls).T)).T,X) +2*rho*beta_ls\n",
    "    aa= np.squeeze(np.asarray(pp))\n",
    "    return aa\n",
    "\n",
    "def ridge_reg_hess(beta_ls,X,Y, rho):\n",
    "    ss=2*(np.dot(np.transpose(X),X) + rho*np.identity(X.shape[1]))\n",
    "    return ss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenidas la ecuaciones, procedemos a utilizar la función *minimize* con el solver **SLSQP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 393.0171289349598\n",
      "            Iterations: 12\n",
      "            Function evaluations: 165\n",
      "            Gradient evaluations: 12\n",
      "time elapsed= 0.025782715190871386\n",
      "[-0.01157493  0.36184416  0.21667032 -0.07063481  0.36681382 -0.14750638\n",
      " -0.00769331  0.07728442 -0.03988624]\n"
     ]
    }
   ],
   "source": [
    "rho = 2\n",
    "beta_ls0 = np.zeros(X.shape[1])\n",
    "time_start = time.clock()\n",
    "res = minimize(ridge_reg_brute, beta_ls0, args=(X, Y, rho), method='SLSQP',  options={'disp': True})\n",
    "time_elapsed = (time.clock() - time_start) \n",
    "print('time elapsed=',time_elapsed)\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2085, 1)\n",
      "18.8164633569\n",
      "Error= 0.00522012206213\n"
     ]
    }
   ],
   "source": [
    "res.x = res.x.reshape(1,9)\n",
    "error = (np.dot(X,(res.x).T)- Y)\n",
    "print(error.shape)\n",
    "error_MSE =  np.sum(error*error)/(len(X))\n",
    "print(error_MSE*100)\n",
    "print('Error=',np.linalg.norm(np.transpose(beta_ls_exact)-res.x,ord=2)/np.linalg.norm(res.x,ord=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este Modelo obtenemos un **MSE** del **18.81%** muy parecido a la solución análitica, también se puede notar que que la diferencia entres los betas es mínima de alrededor un **0.5%** de error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el Método **Powell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 393.061581\n",
      "         Iterations: 11\n",
      "         Function evaluations: 939\n",
      "time elapsed= 0.14386274793650955\n",
      "[[-0.01099327  0.35650305  0.22309232 -0.07262772  0.36388209 -0.1489435\n",
      "  -0.00706378  0.07946284 -0.04327138]]\n"
     ]
    }
   ],
   "source": [
    "rho = 2\n",
    "beta_ls0 = np.zeros(X.shape[1])\n",
    "time_start = time.clock()\n",
    "#run your code\n",
    "                   \n",
    "res = minimize(ridge_reg_brute, beta_ls0, args=(X, Y, rho), method='Powell', options={'disp': True})\n",
    "time_elapsed = (time.clock() - time_start) \n",
    "print('time elapsed=',time_elapsed)\n",
    "print(res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  18.81877211\n",
      "Error= 0.0170556073873\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res.x = res.x.reshape(1,9)\n",
    "error = (np.dot(X,(res.x).T)- Y)\n",
    "error_MSE =  np.sum(error.T*error)/(len(X))\n",
    "print(\"MSE: \",error_MSE*100)\n",
    "print('Error=',np.linalg.norm(np.transpose(beta_ls_exact)-res.x,ord=2)/np.linalg.norm(res.x,ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método **Nelder-Mead:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 414.324787\n",
      "         Iterations: 1209\n",
      "         Function evaluations: 1770\n",
      "time elapsed= 0.17701092831703136\n",
      "[-0.04609012  0.28978093  0.12053211 -0.151444    0.30599049 -0.27714508\n",
      " -0.09662098 -0.03876434  0.18367621]\n"
     ]
    }
   ],
   "source": [
    "rho = 2\n",
    "beta_ls0 = np.zeros(X.shape[1])\n",
    "time_start = time.clock()\n",
    "#run your code\n",
    "                   \n",
    "res = minimize(ridge_reg_brute, beta_ls0, args=(X, Y, rho), method='Nelder-Mead', options={'disp': True})\n",
    "time_elapsed = (time.clock() - time_start) \n",
    "print('time elapsed=',time_elapsed)\n",
    "print(res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  19.8392151834\n",
      "Error= 0.585133766434\n"
     ]
    }
   ],
   "source": [
    "res.x = res.x.reshape(1,9)\n",
    "error = (np.dot(X,(res.x).T)- Y)\n",
    "error_MSE =  np.sum(error*error)/(len(X))\n",
    "print(\"MSE: \",error_MSE*100)\n",
    "print('Error=',np.linalg.norm(np.transpose(beta_ls_exact)-res.x,ord=2)/np.linalg.norm(res.x,ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos como los parámetros direfieren un poco más y el **MSE** asciende hasta **19.83%**, además este método toma **1209** iteraciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método **Newton-CG:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 393.017129\n",
      "         Iterations: 13\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 26\n",
      "         Hessian evaluations: 13\n",
      "time elapsed= 0.017517852957098512\n",
      "[-0.01157431  0.36184528  0.21667022 -0.07063424  0.36681453 -0.14750531\n",
      " -0.00769267  0.0772854  -0.03988807]\n"
     ]
    }
   ],
   "source": [
    "rho = 2\n",
    "beta_ls0 = np.zeros(X.shape[1])\n",
    "time_start = time.clock()\n",
    "res = minimize(ridge_reg_brute, beta_ls0, args=(X, Y, rho), method='Newton-CG', jac=ridge_reg_der, \n",
    "               hess=ridge_reg_hess, options={'disp': True,'xtol': 1e-15})\n",
    "time_elapsed = (time.clock() - time_start) \n",
    "print('time elapsed=',time_elapsed)\n",
    "print(res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  18.8164632451\n",
      "Error= 0.00521607668832\n"
     ]
    }
   ],
   "source": [
    "res.x = res.x.reshape(1,9)\n",
    "error = (np.dot(X,(res.x).T)- Y)\n",
    "error_MSE =  np.sum(error*error)/(len(X))\n",
    "print(\"MSE: \",error_MSE*100)\n",
    "print('Error=',np.linalg.norm(np.transpose(beta_ls_exact)-res.x,ord=2)/np.linalg.norm(res.x,ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más o menos igual que los anteriores en terminos de errores, pero este método tiene un mejor tiempo de ejecución, menos iteraciones y menos evaluaciones del gradiente y del hessiano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método **dogleg:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 393.017129\n",
      "         Iterations: 1\n",
      "         Function evaluations: 2\n",
      "         Gradient evaluations: 2\n",
      "         Hessian evaluations: 1\n",
      "time elapsed= 0.0038792633094999474\n",
      "[-0.01157431  0.36184528  0.21667022 -0.07063424  0.36681453 -0.14750531\n",
      " -0.00769267  0.0772854  -0.03988807]\n"
     ]
    }
   ],
   "source": [
    "rho = 2\n",
    "beta_ls0 = np.zeros(X.shape[1])\n",
    "time_start = time.clock()\n",
    "res = minimize(ridge_reg_brute, beta_ls0, args=(X, Y, rho), method='dogleg', jac=ridge_reg_der, \n",
    "               hess=ridge_reg_hess, options={'disp': True})\n",
    "time_elapsed = (time.clock() - time_start) \n",
    "print('time elapsed=',time_elapsed)\n",
    "print(res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  18.8164632451\n",
      "Error= 0.00521607668833\n"
     ]
    }
   ],
   "source": [
    "res.x = res.x.reshape(1,9)\n",
    "error = (np.dot(X,(res.x).T)- Y)\n",
    "error_MSE =  np.sum(error*error)/(len(X))\n",
    "print(\"MSE: \",error_MSE*100)\n",
    "print('Error=',np.linalg.norm(np.transpose(beta_ls_exact)-res.x,ord=2)/np.linalg.norm(res.x,ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esté método es el **más rápido** de todos, con sola  iteración, 2 evaluaciones del gradiente y  sola evaluación del hessiano, con respecto al **MSE** presenta igual desempeño que los demás, por lo que de elegir un método sería este."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Si comparamos con el paquete _sklearn_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01158656  0.36336376  0.21877346 -0.06865408  0.36890455 -0.14641156\n",
      "  -0.00616682  0.07762911 -0.04437423]]\n",
      "Error= 0.00533164029906\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "regr = linear_model.LinearRegression(fit_intercept=False)\n",
    "regr.fit(X,Y)\n",
    "print(regr.coef_)\n",
    "print('Error=',np.linalg.norm(np.transpose(beta_ls_exact)-regr.coef_,ord=2)/np.linalg.norm(regr.coef_,ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Estimar el valor de los coeﬁcientes de la regresión implementando:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método del Gradiente\n",
    "\n",
    " $\\rightarrow$ From an initial iterate $x_0$\n",
    "\n",
    "$\\rightarrow$ Compute search (descent) directions $p_k=-\\nabla f(x_k)$\n",
    "\n",
    "$\\rightarrow$ Far from the solution, compute a steplength $\\alpha_k>0$\n",
    "\n",
    "$\\rightarrow$ Movement:\n",
    "$$x_{k+1} = x_k + \\alpha_k\\ p_k$$\n",
    "Until convergence to a local solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed= 0.11513377114897594\n",
      "iterations 156\n",
      "392.669371317\n",
      "[-0.01158019  0.36258754  0.21768264 -0.06967451  0.36783479 -0.1469776\n",
      " -0.00695324  0.07744844 -0.04206313]\n",
      "[[-0.0115803   0.36259694  0.21770513 -0.06965771  0.36784859 -0.14696702\n",
      "  -0.00694018  0.07745367 -0.04210075]]\n",
      "Tolerance= 0.00921564942466\n",
      "error= 8.94886878951e-05\n"
     ]
    }
   ],
   "source": [
    "(a,b)=X.shape\n",
    "beta_lsg=np.zeros(b) #initial value for beta\n",
    "alpha=0.01\n",
    "n_iter=50000 #maximim nunber iteration\n",
    "OF_iter=np.zeros(n_iter)\n",
    "tol_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "i=0;\n",
    "rho = 1;\n",
    "tol=10000;\n",
    "epsilon=1e-2;\n",
    "time_start = time.clock()\n",
    "while (i <= n_iter-2) and (tol>epsilon):\n",
    "    i=i+1\n",
    "    \n",
    "    grad=ridge_reg_der(beta_lsg,X,Y, rho)#gradient vector\n",
    "    ddirect=-grad#descent direction\n",
    "    ####Armijo rule to adjust alpha########\n",
    "    sigma = 0.1\n",
    "    beta= 0.1\n",
    "    alpha=1\n",
    "    while (ridge_reg_brute(beta_lsg+alpha*ddirect,X,Y, rho) > \n",
    "           ridge_reg_brute(beta_lsg,X,Y, rho)+alpha*sigma*np.dot(grad,ddirect)):\n",
    "        alpha=alpha*beta\n",
    "    #######################################\n",
    "    beta_lsg=beta_lsg+alpha*ddirect\n",
    "    OF_iter[i]=ridge_reg_brute(beta_lsg, X, Y, rho)\n",
    "    #tol=np.absolute((OF_iter[i]-OF_iter[i-1])/OF_iter[i]) #tolerance\n",
    "    tol=np.linalg.norm(grad,ord=2)\n",
    "    tol_iter[i]=tol\n",
    "    alpha_iter[i]=alpha\n",
    "\n",
    "time_elapsed = (time.clock() - time_start) \n",
    "print('time elapsed=',time_elapsed)\n",
    "print('iterations',i)\n",
    "print(OF_iter[i])\n",
    "print(beta_lsg)\n",
    "print(np.transpose(beta_ls_exact))\n",
    "print('Tolerance=',tol)\n",
    "print('error=',np.linalg.norm(np.transpose(beta_ls_exact)-beta_lsg,ord=2)/np.linalg.norm(beta_lsg,ord=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con este método observamos que se realizaron **156** iteranciones y los coeficientes $ \\beta $ son cercanos a la solución exacta y el tiempo de ejecución es mínimo, el **MSE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  18.8163456334\n",
      "Error= 8.94886878951e-05\n"
     ]
    }
   ],
   "source": [
    "beta_lsg= beta_lsg.reshape(1,9)\n",
    "error = (np.dot(X,(beta_lsg).T)- Y)\n",
    "error_MSE =  np.sum(error*error)/(len(X))\n",
    "print(\"MSE: \",error_MSE*100)\n",
    "print('Error=',np.linalg.norm(np.transpose(beta_ls_exact)-beta_lsg,ord=2)/np.linalg.norm(beta_lsg,ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con el Método de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed= 0.0010761047742562369\n",
      "iterations 2\n",
      "392.6693711\n",
      "[-0.0115803   0.36259694  0.21770513 -0.06965771  0.36784859 -0.14696702\n",
      " -0.00694018  0.07745367 -0.04210075]\n",
      "[[-0.0115803   0.36259694  0.21770513 -0.06965771  0.36784859 -0.14696702\n",
      "  -0.00694018  0.07745367 -0.04210075]]\n",
      "Tolerance= 1.06740173413e-11\n",
      "error= 3.77558506323e-15\n"
     ]
    }
   ],
   "source": [
    "(a,b)=X.shape\n",
    "beta_lsg=np.zeros(b) #initial value for beta\n",
    "alpha=0.01\n",
    "n_iter=50000 #maximim nunber iteration\n",
    "OF_iter=np.zeros(n_iter)\n",
    "tol_iter=np.zeros(n_iter)\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "i=0;\n",
    "rho = 1;\n",
    "tol=10000;\n",
    "epsilon=1e-2;\n",
    "time_start = time.clock()\n",
    "while (i <= n_iter-2) and (tol>epsilon):\n",
    "    i=i+1\n",
    "    \n",
    "    grad=ridge_reg_der(beta_lsg,X,Y, rho)#gradient vector\n",
    "    hess=ridge_reg_hess(beta_lsg,X,Y, rho)\n",
    "    ddirect=-np.dot(np.linalg.inv(hess),grad)\n",
    "    ####Armijo rule to adjust alpha########\n",
    "    sigma = 0.1\n",
    "    beta= 0.1\n",
    "    alpha=1\n",
    "    while (ridge_reg_brute(beta_lsg+alpha*ddirect,X,Y, rho) > \n",
    "           ridge_reg_brute(beta_lsg,X,Y, rho)+alpha*sigma*np.dot(grad,ddirect)):\n",
    "        alpha=alpha*beta\n",
    "    #######################################\n",
    "    beta_lsg=beta_lsg+alpha*ddirect\n",
    "    OF_iter[i]=ridge_reg_brute(beta_lsg, X, Y, rho)\n",
    "    #tol=np.absolute((OF_iter[i]-OF_iter[i-1])/OF_iter[i]) #tolerance\n",
    "    tol=np.linalg.norm(grad,ord=2)\n",
    "    tol_iter[i]=tol\n",
    "    alpha_iter[i]=alpha\n",
    "\n",
    "time_elapsed = (time.clock() - time_start) \n",
    "print('time elapsed=',time_elapsed)\n",
    "print('iterations',i)\n",
    "print(OF_iter[i])\n",
    "print(beta_lsg)\n",
    "print(np.transpose(beta_ls_exact))\n",
    "print('Tolerance=',tol)\n",
    "print('error=',np.linalg.norm(np.transpose(beta_ls_exact)-beta_lsg,ord=2)/np.linalg.norm(beta_lsg,ord=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método de Newton toma 2 iteraciones para converger, con un error despreciable cuando comparamos los coeficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimar el valor de los coeﬁcientes de la regresión implementando el método del gradiente estocástico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este método se basa en actualizar los gradientes por cada muestra del dataset, se supone que con pocas iteraciones bastarían para llegar a una solución _aceptable_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-357-396c70555d1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta_lsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrho\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbeta_lsg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mddirect\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;31m#descent direction\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mbeta_lsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbeta_lsg\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mddirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "(a,b)=X.shape\n",
    "beta_lsg=np.zeros(b) #initial valuer beta\n",
    "alpha=1e-2\n",
    "n_iter=50000 #maximim nunber iteration\n",
    "OF_iter=np.zeros(n_iter)\n",
    "tol_iter=np.zeros(n_iter)\n",
    "tol_iter_aux=np.zeros(len(Y))\n",
    "alpha_iter=np.zeros(n_iter)\n",
    "i=0;\n",
    "rho = 0.2;\n",
    "grad_old = 0\n",
    "tol=10000;\n",
    "epsilon=1e-2;\n",
    "time_start = time.clock()\n",
    "while (i <= n_iter-2) and (tol>epsilon):\n",
    "    i=i+1\n",
    " ####Armijo rule to adjust alpha########\n",
    "    sigma = 0.1\n",
    "    beta= 0.1\n",
    "\n",
    "    while (ridge_reg_brute(beta_lsg+alpha*ddirect,X,Y, rho) > ridge_reg_brute(beta_lsg,X,Y, rho)+alpha*sigma*np.dot(grad,ddirect)):    alpha=alpha*beta\n",
    "   #######################################\n",
    "\n",
    "    index_t = np.random.permutation(len(Y))\n",
    "    for index in index_t:\n",
    "        y = (Y[index])\n",
    "        x =(X[index,:])\n",
    "        grad=-2*np.dot((y*np.identity(X.shape[1])-np.dot(x,(beta_lsg).T)).T,x) +2*rho*beta_lsg\n",
    "        ddirect=-grad#descent direction\n",
    "        beta_lsg=beta_lsg+alpha*ddirect\n",
    "        tol_iter_aux[index]=np.linalg.norm(grad,ord=2)\n",
    "\n",
    "    OF_iter[i]=ridge_reg_brute(beta_lsg, x, y, rho)\n",
    "    #tol=np.linalg.norm(grad,ord=2)\n",
    "    tol =np.linalg.norm(grad, ord=2)\n",
    "    grad_old = grad\n",
    "    tol_iter[i]=tol\n",
    "    alpha_iter[i]=alpha   \n",
    " \n",
    "\n",
    "\n",
    "time_elapsed = (time.clock() - time_start) \n",
    "print('time elapsed=',time_elapsed)\n",
    "print('iterations',i)\n",
    "#print(OF_iter[i])\n",
    "#print(beta_lsg)\n",
    "print(np.transpose(beta_ls_exact))\n",
    "print(beta_lsg)\n",
    "print('Tolerance=',tol)\n",
    "print('error=',np.linalg.norm(np.transpose(beta_ls_exact)-beta_lsg,ord=2)/np.linalg.norm(beta_lsg,ord=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se Observa este método es algo inestable y presenta alguno problemas de convergencia, pero puede ser muy rapido para millones de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#plt.plot(OF_iter[1:i])\n",
    "#plt.figure()\n",
    "as_fig = plt.plot((np.log(tol_iter)))\n",
    "plt.show()\n",
    "#plt.plot(alpha_iter[1:i])\n",
    "print(tol_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En resumen:\n",
    " * En el análisis de modelos de regresión se deben ajustar convenientement los _hiperparametros_ como $ \\lambda, \\beta, \\rho ,$ etc.\n",
    " * En el ejericio notamos que muchos métodos se aproximan a la solución exacta, sin embargo la elección del método depende del también del tiempo de ejecución y del MSE.\n",
    " * Con el método de Newton obtuvimos aún mejores resultados y con menos tiempo de ejecución que cuando se compara con alguno métodos de paquetes de _python_.\n",
    " * El método del gradiente estocástico es viable cuando se tiene millones de muestras,  es decir en la que priorizamos velocidad en detrimento de exactitud."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
